{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2210748c-d451-45ca-8326-2e4c415d1aaa",
   "metadata": {},
   "source": [
    "# 1. Start Spark Session and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70977c96-5c79-47d1-80ca-2bb59ea0c09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hadoop/spark-3.5.4/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jj/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jj/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ce135d54-381e-446d-a23d-cef0da19c2f1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 583ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ce135d54-381e-446d-a23d-cef0da19c2f1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "25/04/17 20:30:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/17 20:30:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/04/17 20:30:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType, BooleanType\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "kafka_package = \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\"\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDelayStreamingPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82392924-ceaa-414c-8339-8884f1cd04ef",
   "metadata": {},
   "source": [
    "# 2. Load Models & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af595248-607b-4b80-a080-761289500d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pipeline_path = \"./flight_delay_pipeline_model\"\n",
    "model_path = \"./flight_delay_rf_model\"\n",
    "\n",
    "loaded_pipeline_model = PipelineModel.load(pipeline_path)\n",
    "loaded_rf_model = RandomForestClassificationModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2003a1-96f7-493f-93de-a8226beac97f",
   "metadata": {},
   "source": [
    "# 3. Create Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0c8189-7060-4ba7-98ab-e82d1ec51892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_topic = \"flight_data_stream\"\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09545cd-e15c-4175-b6b5-39fc74db112b",
   "metadata": {},
   "source": [
    "# 4. Prepare for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7678b0c6-60cf-4d04-bdab-97b505a5b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- AIRLINE_CODE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- DEP_HOUR: integer (nullable = true)\n",
      " |-- DEP_MINUTE: integer (nullable = true)\n",
      " |-- ARR_HOUR: integer (nullable = true)\n",
      " |-- ARR_MINUTE: integer (nullable = true)\n",
      " |-- DEP_DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- DEP_MONTH: integer (nullable = true)\n",
      " |-- DEP_DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DEP_WEEK_OF_YEAR: integer (nullable = true)\n",
      " |-- IS_WEEKEND: integer (nullable = false)\n",
      " |-- DISTANCE_PER_MINUTE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"FL_DATE\", StringType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"AIRLINE_CODE\", StringType(), True),\n",
    "    StructField(\"ORIGIN\", StringType(), True),\n",
    "    StructField(\"DEST\", StringType(), True),\n",
    "    StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
    "    StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
    "    StructField(\"CRS_ELAPSED_TIME\", DoubleType(), True),\n",
    "    StructField(\"DISTANCE\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "parsed_stream_df = kafka_df \\\n",
    "    .select(F.from_json(F.col(\"value\"), json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"FL_DATE\", F.to_date(F.col(\"FL_DATE\"), \"yyyy-MM-dd\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_HOUR\", (F.col(\"CRS_DEP_TIME\") / 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_MINUTE\", (F.col(\"CRS_DEP_TIME\") % 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"ARR_HOUR\", (F.col(\"CRS_ARR_TIME\") / 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"ARR_MINUTE\", (F.col(\"CRS_ARR_TIME\") % 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_DAY_OF_WEEK\", F.dayofweek(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_MONTH\", F.month(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_DAY_OF_MONTH\", F.dayofmonth(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_WEEK_OF_YEAR\", F.weekofyear(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"IS_WEEKEND\", F.when(F.col(\"DEP_DAY_OF_WEEK\").isin([1, 7]), 1).otherwise(0))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DISTANCE_PER_MINUTE\", F.col(\"DISTANCE\") / (F.col(\"CRS_ELAPSED_TIME\") + 1e-6))\n",
    "\n",
    "\n",
    "feature_columns = [\n",
    "    \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_ELAPSED_TIME\", \"DISTANCE\",\n",
    "    \"DEP_HOUR\", \"DEP_MINUTE\", \"ARR_HOUR\", \"ARR_MINUTE\",\n",
    "    \"DEP_DAY_OF_WEEK\", \"DEP_MONTH\", \"DEP_DAY_OF_MONTH\", \"DEP_WEEK_OF_YEAR\",\n",
    "    \"IS_WEEKEND\", \"DISTANCE_PER_MINUTE\"\n",
    "]\n",
    "parsed_stream_df = parsed_stream_df.dropna(subset=feature_columns + [\"FL_DATE\"])\n",
    "parsed_stream_df.printSchema()\n",
    "\n",
    "processed_stream_df = loaded_pipeline_model.transform(parsed_stream_df)\n",
    "\n",
    "predictions_df = loaded_rf_model.transform(processed_stream_df)\n",
    "\n",
    "output_df = predictions_df.select(\n",
    "    \"FL_DATE\",\n",
    "    \"AIRLINE_CODE\",\n",
    "    \"ORIGIN\",\n",
    "    \"DEST\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ")\n",
    "\n",
    "output_df = output_df.withColumn(\n",
    "    \"Prediction_Label\",\n",
    "     F.when(F.col(\"prediction\") == 1, \"Severe Delay Predicted\")\n",
    "     .otherwise(\"No Severe Delay Predicted\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6130b6fc-f996-4a04-abb3-43bb8024943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- AIRLINE_CODE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- Prediction_Label: string (nullable = false)\n",
      " |-- Probability_Severe_Delay: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 20:34:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fba00bd5-233f-493f-9edf-6a46ddc2cd76. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/17 20:34:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/17 20:34:06 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------------+------+----+------------+----------------+------------------------+\n",
      "|FL_DATE|AIRLINE_CODE|ORIGIN|DEST|CRS_DEP_TIME|Prediction_Label|Probability_Severe_Delay|\n",
      "+-------+------------+------+----+------------+----------------+------------------------+\n",
      "+-------+------------+------+----+------------+----------------+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jj/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jj/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "prob_udf = F.udf(lambda prob: float(prob[1]), DoubleType())\n",
    "output_df = output_df.withColumn(\"Probability_Severe_Delay\", prob_udf(F.col(\"probability\")))\n",
    "\n",
    "output_df.printSchema()\n",
    "\n",
    "query = output_df \\\n",
    "    .select(\"FL_DATE\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"Prediction_Label\", \"Probability_Severe_Delay\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Streaming query interrupted by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a94a4ee-0fde-4a23-9f29-27aa499c5102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
