{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2210748c-d451-45ca-8326-2e4c415d1aaa",
   "metadata": {},
   "source": [
    "# 1. Start Spark Session and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70977c96-5c79-47d1-80ca-2bb59ea0c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import system and Spark initialization tools\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, DateType, BooleanType\n",
    "\n",
    "# ml libraries\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "# load kafka and mongo packages\n",
    "# this was our attempt at getting mongo working however we ended up using parquet\n",
    "kafka_package = \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1\"\n",
    "mongo_package = \"org.mongodb.spark:mongo-spark-connector_2.13:10.2.1\"\n",
    "\n",
    "# Build and configure the SparkSession:\n",
    "#  - Name the app\n",
    "#  - Allocate memory for driver and executors\n",
    "#  - Include the Kafka connector package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDelayStreamingPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{kafka_package},{mongo_package}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82392924-ceaa-414c-8339-8884f1cd04ef",
   "metadata": {},
   "source": [
    "# 2. Load Models & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af595248-607b-4b80-a080-761289500d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Define the filesystem path where the saved preprocessing pipeline is stored\n",
    "# 2. Define the filesystem path where the trained GBT Classifier model is stored\n",
    "pipeline_path = \"./flight_delay_gbt_pipeline_model\"\n",
    "model_path = \"./flight_delay_gbt_model\"\n",
    "\n",
    "# 3. Load the preprocessing PipelineModel (e.g., StringIndexers, VectorAssembler, scalers)\n",
    "# 4. Load the trained GBTClassificationModel for making predictions\n",
    "loaded_pipeline_model = PipelineModel.load(pipeline_path)\n",
    "loaded_rf_model = GBTClassificationModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2003a1-96f7-493f-93de-a8226beac97f",
   "metadata": {},
   "source": [
    "# 3. Create Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c8189-7060-4ba7-98ab-e82d1ec51892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Kafka topic and bootstrap server(s)\n",
    "kafka_topic = \"flight_data_stream\"\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "\n",
    "# 2. Create a streaming DataFrame by reading from Kafka\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# 3. Kafka “value” column comes in as binary; cast it to string for JSON parsing later\n",
    "kafka_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# 4. Print the schema to verify the DataFrame structure\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09545cd-e15c-4175-b6b5-39fc74db112b",
   "metadata": {},
   "source": [
    "# 4. Prepare for Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7678b0c6-60cf-4d04-bdab-97b505a5b505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- AIRLINE_CODE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- DEP_HOUR: integer (nullable = true)\n",
      " |-- DEP_MINUTE: integer (nullable = true)\n",
      " |-- ARR_HOUR: integer (nullable = true)\n",
      " |-- ARR_MINUTE: integer (nullable = true)\n",
      " |-- DEP_DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- DEP_MONTH: integer (nullable = true)\n",
      " |-- DEP_DAY_OF_MONTH: integer (nullable = true)\n",
      " |-- DEP_WEEK_OF_YEAR: integer (nullable = true)\n",
      " |-- IS_WEEKEND: integer (nullable = false)\n",
      " |-- DISTANCE_PER_MINUTE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the expected JSON schema for incoming Kafka messages\n",
    "json_schema = StructType([\n",
    "    StructField(\"FL_DATE\", StringType(), True),\n",
    "    StructField(\"AIRLINE\", StringType(), True),\n",
    "    StructField(\"AIRLINE_CODE\", StringType(), True),\n",
    "    StructField(\"ORIGIN\", StringType(), True),\n",
    "    StructField(\"DEST\", StringType(), True),\n",
    "    StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
    "    StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
    "    StructField(\"CRS_ELAPSED_TIME\", DoubleType(), True),\n",
    "    StructField(\"DISTANCE\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# 2. Parse the JSON payload from the Kafka 'value' column into individual columns\n",
    "parsed_stream_df = kafka_df \\\n",
    "    .select(F.from_json(F.col(\"value\"), json_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# 3. Feature engineering on the streaming DataFrame\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"FL_DATE\", F.to_date(F.col(\"FL_DATE\"), \"yyyy-MM-dd\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_HOUR\", (F.col(\"CRS_DEP_TIME\") / 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_MINUTE\", (F.col(\"CRS_DEP_TIME\") % 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"ARR_HOUR\", (F.col(\"CRS_ARR_TIME\") / 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"ARR_MINUTE\", (F.col(\"CRS_ARR_TIME\") % 100).cast(\"integer\"))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_DAY_OF_WEEK\", F.dayofweek(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_MONTH\", F.month(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_DAY_OF_MONTH\", F.dayofmonth(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DEP_WEEK_OF_YEAR\", F.weekofyear(F.col(\"FL_DATE\")))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"IS_WEEKEND\", F.when(F.col(\"DEP_DAY_OF_WEEK\").isin([1, 7]), 1).otherwise(0))\n",
    "parsed_stream_df = parsed_stream_df.withColumn(\"DISTANCE_PER_MINUTE\", F.col(\"DISTANCE\") / (F.col(\"CRS_ELAPSED_TIME\") + 1e-6))\n",
    "\n",
    "# 4. Select the features you’ll use in your model and drop any rows missing them\n",
    "feature_columns = [\n",
    "    \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_ELAPSED_TIME\", \"DISTANCE\",\n",
    "    \"DEP_HOUR\", \"DEP_MINUTE\", \"ARR_HOUR\", \"ARR_MINUTE\",\n",
    "    \"DEP_DAY_OF_WEEK\", \"DEP_MONTH\", \"DEP_DAY_OF_MONTH\", \"DEP_WEEK_OF_YEAR\",\n",
    "    \"IS_WEEKEND\", \"DISTANCE_PER_MINUTE\"\n",
    "]\n",
    "parsed_stream_df = parsed_stream_df.dropna(subset=feature_columns + [\"FL_DATE\"])\n",
    "\n",
    "# 5. Print schema to verify all fields and types\n",
    "parsed_stream_df.printSchema()\n",
    "\n",
    "# 6. Apply preprocessing pipeline (e.g., StringIndexers, VectorAssembler, scaling)\n",
    "processed_stream_df = loaded_pipeline_model.transform(parsed_stream_df)\n",
    "\n",
    "# 7. Generate predictions using the pre‑trained Random Forest model\n",
    "predictions_df = loaded_rf_model.transform(processed_stream_df)\n",
    "\n",
    "# 8. Select only the columns you want to output downstream\n",
    "output_df = predictions_df.select(\n",
    "    \"FL_DATE\",\n",
    "    \"AIRLINE_CODE\",\n",
    "    \"ORIGIN\",\n",
    "    \"DEST\",\n",
    "    \"CRS_DEP_TIME\",\n",
    "    \"CRS_ARR_TIME\",\n",
    "    \"DISTANCE\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ")\n",
    "\n",
    "# 9. Map the numeric prediction to a human‑readable label\n",
    "output_df = output_df.withColumn(\n",
    "    \"Prediction_Label\",\n",
    "     F.when(F.col(\"prediction\") == 1, \"Severe Delay Predicted\")\n",
    "     .otherwise(\"No Severe Delay Predicted\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after you run this, it will continue running while streaming the data to the website\n",
    "# make sure kafka producer and flask app are running in two terminals\n",
    "\n",
    "# 1. Define a UDF to pull out the probability of the “severe delay” class (index 1 of the vector)\n",
    "prob_udf = F.udf(lambda prob: float(prob[1]), DoubleType())\n",
    "\n",
    "# 2. Add a new column with the extracted probability\n",
    "output_df = output_df.withColumn(\"Probability_Severe_Delay\", prob_udf(F.col(\"probability\")))\n",
    "\n",
    "# 3. Print the schema to verify that Probability_Severe_Delay was added correctly\n",
    "output_df.printSchema()\n",
    "\n",
    "# 4. Define the MongoDB connection string and database/collection names\n",
    "mongo_uri = \"mongodb://127.0.0.1/flightdb.flight_predictions\"\n",
    "\n",
    "# 5. Build and start the streaming query:\n",
    "mongo_query = output_df \\\n",
    "    .select(\"FL_DATE\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"Prediction_Label\", \"Probability_Severe_Delay\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.connection.uri\", mongo_uri) \\\n",
    "    .option(\"checkpointLocation\", \"./mongodb_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "# Console output\n",
    "console_query = output_df \\\n",
    "    .select(\"FL_DATE\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"Prediction_Label\", \"Probability_Severe_Delay\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# File output\n",
    "file_output_path = \"./streaming_predictions_output\"\n",
    "file_query = output_df \\\n",
    "    .select(\"FL_DATE\", \"AIRLINE_CODE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"Prediction_Label\", \"Probability_Severe_Delay\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", file_output_path) \\\n",
    "    .option(\"checkpointLocation\", \"./streaming_predictions_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for all queries\n",
    "try:\n",
    "    while True:\n",
    "        if any([q.isActive for q in [console_query, file_query, mongo_query]]):\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "except KeyboardInterrupt:\n",
    "    for q in [console_query, file_query, mongo_query]:\n",
    "        q.stop()\n",
    "    print(\"Streaming query interrupted by user.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
